{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === readme ===\n",
    "descrip: initiate dask in grace with SLURM \n",
    "\n",
    "update history: <br>\n",
    "v1.0 DL 2021Mar30\n",
    "\n",
    "extra notes: <br>\n",
    "python virtual env on grace: /scratch/group/ihesp/shared/conda/envs/envMar25/ <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  === import modules ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dask jupyter lab packages\n",
    "from dask.distributed import Client\n",
    "# from dask_jobqueue import LSFCluster # ada uses LSFCluster\n",
    "from dask_jobqueue import SLURMCluster # grace uses SLURMCluster\n",
    "# from dask.distributed import performance_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# === initiate dask on grace ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p medium\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=32\n",
      "#SBATCH --mem=168G\n",
      "#SBATCH -t 03:00:00\n",
      "\n",
      "/scratch/group/ihesp/shared/conda/envs/envMar25/bin/python -m distributed.cli.dask_worker tcp://10.73.1.161:38309 --nthreads 1 --nprocs 32 --memory-limit 5.62GB --name dummy-name --nanny --death-timeout 60 --protocol tcp://\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(cores=32, processes=32, memory=\"180GB\", queue='medium', walltime=\"03:00:00\")\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1) # need this line, can not skip it, 2021Apr20\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#SBATCH -J dask-worker\n",
      "#SBATCH -p medium\n",
      "#SBATCH -n 1\n",
      "#SBATCH --cpus-per-task=10\n",
      "#SBATCH --mem=196G\n",
      "#SBATCH -t 05:00:00\n",
      "\n",
      "/scratch/group/ihesp/shared/conda/envs/envMar25/bin/python -m distributed.cli.dask_worker tcp://10.73.3.113:36498 --nthreads 1 --nprocs 10 --memory-limit 21.00GB --name dummy-name --nanny --death-timeout 60 --protocol tcp://\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cluster = SLURMCluster(cores=10, processes=10, memory=\"210GB\", queue='medium', \n",
    "                       walltime=\"05:00:00\")#, death_timeout=600 # for job submission\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1) # need this line, can not skip it, 2021Apr20\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.73.3.113:36498</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.73.3.113:8787/status' target='_blank'>http://10.73.3.113:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>10</li>\n",
       "  <li><b>Cores: </b>10</li>\n",
       "  <li><b>Memory: </b>210.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.73.3.113:36498' processes=10 threads=10, memory=210.00 GB>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login to grace from terminal and use \"squeue -u dapengli\" to see if the job starts run. \n",
    "# Run codes below after the dask job starts. \n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  === initiate dask on ada ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -n 4\n",
      "#BSUB -M 16000\n",
      "#BSUB -W 01:00\n",
      "#BSUB -R \"span[ptile=4]\"\n",
      "#BSUB -R \"rusage[mem=4000]\"\n",
      "\n",
      "/ihesp/shared/conda/envs/jupyterlab_env/bin/python -m distributed.cli.dask_worker tcp://10.72.17.42:38273 --nthreads 2 --nprocs 2 --memory-limit 8.00GB --name name --nanny --death-timeout 60 --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: Abishek's email on 2020Sep08\n",
    "cluster = LSFCluster(cores=4,processes=2,memory='16GB',interface='ib0',\n",
    "          use_stdin=True,walltime=\"01:00\",job_extra=['-R \"span[ptile=4]\"','-R \"rusage[mem=4000]\"'])\n",
    "# for dask, number of cores = number of threads\n",
    "# For 4GB memory per core, chunk size should be 100-500 MB for optimization, better to be a multiple of 2\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1)\n",
    "# cluster.scale(jobs=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Task exception was never retrieved\n",
      "future: <Task finished coro=<_wrap_awaitable() done, defined at /scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/asyncio/tasks.py:623> exception=RuntimeError('Command exited with non-zero exit code.\\nExit code: 127\\nCommand:\\nbsub< /tmp/job.15957/tmpc7ung6wq.sh 2> /dev/null\\nstdout:\\n\\nstderr:\\n\\n')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/asyncio/tasks.py\", line 630, in _wrap_awaitable\n",
      "    return (yield from awaitable.__await__())\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/distributed/deploy/spec.py\", line 71, in _\n",
      "    await self.start()\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 324, in start\n",
      "    out = await self._submit_job(fn)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/lsf.py\", line 109, in _submit_job\n",
      "    return self._call(piped_cmd, shell=True)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 407, in _call\n",
      "    \"stderr:\\n{}\\n\".format(proc.returncode, cmd_str, out, err)\n",
      "RuntimeError: Command exited with non-zero exit code.\n",
      "Exit code: 127\n",
      "Command:\n",
      "bsub< /tmp/job.15957/tmpc7ung6wq.sh 2> /dev/null\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "\n",
      "\n",
      "tornado.application - ERROR - Exception in callback functools.partial(<bound method IOLoop._discard_future_result of <zmq.eventloop.ioloop.ZMQIOLoop object at 0x2b653778b490>>, <Task finished coro=<SpecCluster._correct_state_internal() done, defined at /scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/distributed/deploy/spec.py:325> exception=RuntimeError('Command exited with non-zero exit code.\\nExit code: 127\\nCommand:\\nbsub< /tmp/job.15957/tmp766fa6t8.sh 2> /dev/null\\nstdout:\\n\\nstderr:\\n\\n')>)\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/tornado/ioloop.py\", line 741, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/tornado/ioloop.py\", line 765, in _discard_future_result\n",
      "    future.result()\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/distributed/deploy/spec.py\", line 360, in _correct_state_internal\n",
      "    await w  # for tornado gen.coroutine support\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/distributed/deploy/spec.py\", line 71, in _\n",
      "    await self.start()\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 324, in start\n",
      "    out = await self._submit_job(fn)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/lsf.py\", line 109, in _submit_job\n",
      "    return self._call(piped_cmd, shell=True)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 407, in _call\n",
      "    \"stderr:\\n{}\\n\".format(proc.returncode, cmd_str, out, err)\n",
      "RuntimeError: Command exited with non-zero exit code.\n",
      "Exit code: 127\n",
      "Command:\n",
      "bsub< /tmp/job.15957/tmp766fa6t8.sh 2> /dev/null\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "\n",
      "\n",
      "Task exception was never retrieved\n",
      "future: <Task finished coro=<_wrap_awaitable() done, defined at /scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/asyncio/tasks.py:623> exception=RuntimeError('Command exited with non-zero exit code.\\nExit code: 127\\nCommand:\\nbsub< /tmp/job.15957/tmpombw0qnu.sh 2> /dev/null\\nstdout:\\n\\nstderr:\\n\\n')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/asyncio/tasks.py\", line 630, in _wrap_awaitable\n",
      "    return (yield from awaitable.__await__())\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/distributed/deploy/spec.py\", line 71, in _\n",
      "    await self.start()\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 324, in start\n",
      "    out = await self._submit_job(fn)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/lsf.py\", line 109, in _submit_job\n",
      "    return self._call(piped_cmd, shell=True)\n",
      "  File \"/scratch/group/ihesp/shared/conda/envs/envMar25/lib/python3.7/site-packages/dask_jobqueue/core.py\", line 407, in _call\n",
      "    \"stderr:\\n{}\\n\".format(proc.returncode, cmd_str, out, err)\n",
      "RuntimeError: Command exited with non-zero exit code.\n",
      "Exit code: 127\n",
      "Command:\n",
      "bsub< /tmp/job.15957/tmpombw0qnu.sh 2> /dev/null\n",
      "stdout:\n",
      "\n",
      "stderr:\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -n 8\n",
      "#BSUB -R \"span[hosts=1]\"\n",
      "#BSUB -M 32000000\n",
      "#BSUB -W 02:00\n",
      "#BSUB -R \"span[ptile=8]\"\n",
      "#BSUB -R \"rusage[mem=4000]\"\n",
      "\n",
      "/scratch/group/ihesp/shared/conda/envs/envMar25/bin/python -m distributed.cli.dask_worker tcp://10.73.131.229:43845 --nthreads 2 --nprocs 4 --memory-limit 8.00GB --name dummy-name --nanny --death-timeout 60 --interface ib0 --protocol tcp://\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: Abishek's email on 2020Sep08\n",
    "cluster = LSFCluster(cores=8,processes=4,memory='32GB',interface='ib0',\n",
    "          use_stdin=True,walltime=\"02:00\",job_extra=['-R \"span[ptile=8]\"','-R \"rusage[mem=4000]\"'])\n",
    "# for dask, number of cores = number of threads\n",
    "# For 4GB memory per core, chunk size should be 100-500 MB for optimization, better to be a multiple of 2\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1)\n",
    "# cluster.scale(jobs=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -n 12\n",
      "#BSUB -M 48000\n",
      "#BSUB -W 01:00\n",
      "#BSUB -R \"span[ptile=12]\"\n",
      "#BSUB -R \"rusage[mem=4000]\"\n",
      "\n",
      "/ihesp/shared/conda/envs/jupyterlab_env/bin/python -m distributed.cli.dask_worker tcp://10.72.13.50:32833 --nthreads 2 --nprocs 6 --memory-limit 8.00GB --name name --nanny --death-timeout 60 --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: Abishek's email on 2020Sep08\n",
    "cluster = LSFCluster(cores=12,processes=6,memory='48GB',interface='ib0',\n",
    "          use_stdin=True,walltime=\"01:00\",job_extra=['-R \"span[ptile=12]\"','-R \"rusage[mem=4000]\"'])\n",
    "# for dask, number of cores = number of threads\n",
    "# For 4GB memory per core, chunk size should be 100-500 MB for optimization, better to be a multiple of 2\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1)\n",
    "# cluster.scale(jobs=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/usr/bin/env bash\n",
      "\n",
      "#BSUB -J dask-worker\n",
      "#BSUB -n 16\n",
      "#BSUB -M 64000\n",
      "#BSUB -W 04:00\n",
      "#BSUB -R \"span[ptile=16]\"\n",
      "#BSUB -R \"rusage[mem=4000]\"\n",
      "\n",
      "/ihesp/shared/conda/envs/jupyterlab_env/bin/python -m distributed.cli.dask_worker tcp://10.72.19.4:34958 --nthreads 2 --nprocs 8 --memory-limit 8.00GB --name name --nanny --death-timeout 60 --interface ib0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reference: Abishek's email on 2020Sep08\n",
    "cluster = LSFCluster(cores=16,processes=8,memory='64GB',interface='ib0',\n",
    "          use_stdin=True,walltime=\"04:00\",job_extra=['-R \"span[ptile=16]\"','-R \"rusage[mem=4000]\"'])\n",
    "# for dask, number of cores = number of threads\n",
    "# For 4GB memory per core, chunk size should be 100-500 MB for optimization, better to be a multiple of 2\n",
    "print(cluster.job_script())\n",
    "cluster.scale(1)\n",
    "# cluster.scale(jobs=1)\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "login to ada from terminal and use \"bjobs -u dapengli\" to see if the job starts run. Run codes below after the dask job starts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://10.72.20.9:35162</li>\n",
       "  <li><b>Dashboard: </b><a href='http://10.72.20.9:8787/status' target='_blank'>http://10.72.20.9:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>0</li>\n",
       "  <li><b>Cores: </b>0</li>\n",
       "  <li><b>Memory: </b>0 B</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://10.72.20.9:35162' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
